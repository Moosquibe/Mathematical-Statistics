{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An example of the Expectation-Maximization (EM) algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider a sample $Y_1, \\dots, Y_n$ from the mixture density\n",
    "\n",
    "$$ (1-p)\\phi_{\\mu_0, 1}(y) + p\\phi_{\\mu_1,1}(y),$$\n",
    "\n",
    "where $\\phi_{\\mu,\\sigma}(y)$ is the Gaussian density with mean $\\mu$ and variance $\\sigma$, to illustrate the EM algorithm used to find the maximum likelihood estimator of the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate dataset\n",
    "\n",
    "We start by generating a dataset consisting of 100 samples with $p=0.3$, $\\mu_0 = 0$ and $\\mu_1 = 10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 100\n",
    "\n",
    "sample = []\n",
    "cl = [] # To store the class indices\n",
    "\n",
    "p = 0.3\n",
    "mu0 = 0\n",
    "mu1 = 10\n",
    "\n",
    "# There is probably a better way to do this...\n",
    "for i in range(n):\n",
    "    u = np.random.rand();\n",
    "    if (u <= 1-p):\n",
    "        sample.append(np.random.normal(mu0,1));\n",
    "        cl.append(0)\n",
    "    else:\n",
    "        sample.append(np.random.normal(mu1,1));\n",
    "        cl.append(1)\n",
    "        \n",
    "data = pd.DataFrame(list(zip(sample,cl)), columns = ['Sample', 'Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.420985</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.484146</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.920703</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.609991</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.080962</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sample  Type\n",
       "0  -0.420985     0\n",
       "1  -0.484146     0\n",
       "2   9.920703     1\n",
       "3  10.609991     1\n",
       "4   0.080962     0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlYAAAHVCAYAAAAzabX0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAEAnwgoAoJP/B32ElRS626dEAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10b63f630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "type1_data = data[data.Type == 1]\n",
    "type0_data = data[data.Type == 0]\n",
    "plt.scatter(type1_data.Sample, np.zeros(len(type1_data)), s = 200, c = 'r');\n",
    "plt.scatter(type0_data.Sample, np.ones(len(type0_data)), s = 200, c = 'b');\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## The iteration rule\n",
    "\n",
    "Now we implement a function carrying out the iterative step using the formulas derived in class. Recall that we first introduced the auxiliary quantity\n",
    "\n",
    "$$\\tau_i^{(j)} = \\frac{p^{(j)}\\phi_{\\mu_1^{(j)},1}(y_i)}{p^{(j)}\\phi_{\\mu_1^{(j)},1}(y_i)+(1-p^{(j)})\\phi_{\\mu_0^{(j)},1}(y_i)}$$\n",
    "\n",
    "in terms of which the update rule takes the form\n",
    "\n",
    "$$p^{(j+1)}=\\frac{1}{n}\\sum_{i=1}^n\\tau_i^{(j)},\\qquad\\mu_0^{(j+1)}=\\frac{\\sum_{i=1}^ny_i(1-\\tau_i^{(j)})}{\\sum_{i=1}^n(1-\\tau_i^{(j)})},\\qquad \\mu_1^{(j+1)}=\\frac{\\sum_{i=1}^ny_i\\tau_i^j}{\\sum_{i=1}^n\\tau_i^j}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tau(p, mu0, mu1, data):\n",
    "    # Computes the tau-factors\n",
    "    a = stats.norm.pdf(data, loc=mu0, scale=1);\n",
    "    b = stats.norm.pdf(data, loc=mu1, scale=1);\n",
    "    \n",
    "    return p * b / (p * b + (1-p) * a)\n",
    "    \n",
    "def update(p_old, mu0_old, mu1_old, y_data):\n",
    "    # Update rule for the iteration\n",
    "    if p_old == 1:\n",
    "        return [1, mu0_old, y_data.mean()]\n",
    "    elif p_old == 0:\n",
    "        return [0, y_data.mean(), mu1_old]\n",
    "    else:\n",
    "        tau_factors = tau(p_old, mu0_old, mu1_old, y_data);\n",
    "        p_new = tau_factors.mean()\n",
    "        mu0_new = np.dot(y_data, 1-tau_factors) / (1-tau_factors).sum()\n",
    "        mu1_new = np.dot(y_data, tau_factors) / tau_factors.sum()\n",
    "        return [p_new, mu0_new, mu1_new]\n",
    "\n",
    "def log_likelihood(p, mu0, mu1, data):\n",
    "    return (np.log((1-p)*stats.norm.pdf(data, loc=mu0, scale=1) + p * stats.norm.pdf(data, loc=mu1, scale=1))).sum();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "initial_theta = [0.1, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial: [0.1, 0, 1] log-Likelihood: -1893.42\n",
      " 1. p: 0.45, mu0: -0.04, mu1: 9.00   log-Likelihood: -231.14 \n",
      " 2. p: 0.40, mu0: 0.04, mu1: 10.12   log-Likelihood: -205.46 \n",
      " 3. p: 0.40, mu0: 0.04, mu1: 10.12   log-Likelihood: -205.46 \n",
      " 4. p: 0.40, mu0: 0.04, mu1: 10.12   log-Likelihood: -205.46 \n",
      " 5. p: 0.40, mu0: 0.04, mu1: 10.12   log-Likelihood: -205.46 \n",
      " 6. p: 0.40, mu0: 0.04, mu1: 10.12   log-Likelihood: -205.46 \n",
      " 7. p: 0.40, mu0: 0.04, mu1: 10.12   log-Likelihood: -205.46 \n",
      " 8. p: 0.40, mu0: 0.04, mu1: 10.12   log-Likelihood: -205.46 \n",
      " 9. p: 0.40, mu0: 0.04, mu1: 10.12   log-Likelihood: -205.46 \n",
      "10. p: 0.40, mu0: 0.04, mu1: 10.12   log-Likelihood: -205.46 \n",
      "11. p: 0.40, mu0: 0.04, mu1: 10.12   log-Likelihood: -205.46 \n",
      "12. p: 0.40, mu0: 0.04, mu1: 10.12   log-Likelihood: -205.46 \n",
      "13. p: 0.40, mu0: 0.04, mu1: 10.12   log-Likelihood: -205.46 \n",
      "14. p: 0.40, mu0: 0.04, mu1: 10.12   log-Likelihood: -205.46 \n",
      "15. p: 0.40, mu0: 0.04, mu1: 10.12   log-Likelihood: -205.46 \n"
     ]
    }
   ],
   "source": [
    "num_of_iter = 15\n",
    "\n",
    "theta = initial_theta\n",
    "ll = log_likelihood(theta[0], theta[1], theta[2], data.Sample)\n",
    "print(\"Initial: {} log-Likelihood: {:.2f}\".format(theta, ll))\n",
    "\n",
    "for i in range(num_of_iter):\n",
    "    theta = update(theta[0], theta[1], theta[2], data.Sample)\n",
    "    ll = log_likelihood(theta[0], theta[1], theta[2], data.Sample)\n",
    "    print(\"{:2d}. p: {:.2f}, mu0: {:.2f}, mu1: {:.2f}   log-Likelihood: {:.2f} \".format(i+1,theta[0],theta[1],theta[2], ll))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the algorithm converges pretty quickly. We coul tweak this implementation by stopping the iteration, when the log-likelihood is not increasing anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-separable data\n",
    "\n",
    "The above situation worked well because the two components of the mixture were well separated. Let us look at the case of non-separable data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = 100\n",
    "\n",
    "sample = []\n",
    "cl = []\n",
    "\n",
    "p = 0.3\n",
    "mu0 = 0\n",
    "mu1 = 2\n",
    "\n",
    "# There is probably a better way to do this...\n",
    "for i in range(n):\n",
    "    u = np.random.rand();\n",
    "    if (u <= 1-p):\n",
    "        sample.append(np.random.normal(mu0,1));\n",
    "        cl.append(0)\n",
    "    else:\n",
    "        sample.append(np.random.normal(mu1,1));\n",
    "        cl.append(1)\n",
    "        \n",
    "data = pd.DataFrame(list(zip(sample,cl)), columns = ['Sample', 'Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlYAAAHVCAYAAAAzabX0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10fc8f390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "type1_data = data[data.Type == 1]\n",
    "type0_data = data[data.Type == 0]\n",
    "plt.scatter(type1_data.Sample, np.zeros(len(type1_data)), s = 200, c = 'r');\n",
    "plt.scatter(type0_data.Sample, np.ones(len(type0_data)), s = 200, c = 'b');\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial: [0.1, -1, 1] log-Likelihood: -213.39\n",
      " 1. p: 0.32, mu0: -0.28, mu1: 1.72   log-Likelihood: -169.40 \n",
      " 2. p: 0.32, mu0: -0.28, mu1: 1.72   log-Likelihood: -169.40 \n",
      " 3. p: 0.32, mu0: -0.28, mu1: 1.71   log-Likelihood: -169.40 \n",
      " 4. p: 0.32, mu0: -0.28, mu1: 1.71   log-Likelihood: -169.40 \n",
      " 5. p: 0.32, mu0: -0.28, mu1: 1.71   log-Likelihood: -169.40 \n",
      " 6. p: 0.32, mu0: -0.29, mu1: 1.71   log-Likelihood: -169.40 \n",
      " 7. p: 0.32, mu0: -0.29, mu1: 1.71   log-Likelihood: -169.40 \n",
      " 8. p: 0.32, mu0: -0.29, mu1: 1.71   log-Likelihood: -169.40 \n",
      " 9. p: 0.32, mu0: -0.29, mu1: 1.71   log-Likelihood: -169.40 \n",
      "10. p: 0.32, mu0: -0.29, mu1: 1.71   log-Likelihood: -169.40 \n",
      "11. p: 0.32, mu0: -0.29, mu1: 1.71   log-Likelihood: -169.40 \n",
      "12. p: 0.32, mu0: -0.29, mu1: 1.71   log-Likelihood: -169.40 \n",
      "13. p: 0.32, mu0: -0.29, mu1: 1.71   log-Likelihood: -169.40 \n",
      "14. p: 0.32, mu0: -0.29, mu1: 1.71   log-Likelihood: -169.40 \n",
      "15. p: 0.32, mu0: -0.29, mu1: 1.70   log-Likelihood: -169.40 \n",
      "16. p: 0.32, mu0: -0.29, mu1: 1.70   log-Likelihood: -169.40 \n",
      "17. p: 0.32, mu0: -0.29, mu1: 1.70   log-Likelihood: -169.40 \n",
      "18. p: 0.32, mu0: -0.29, mu1: 1.70   log-Likelihood: -169.40 \n",
      "19. p: 0.32, mu0: -0.29, mu1: 1.70   log-Likelihood: -169.40 \n",
      "20. p: 0.32, mu0: -0.29, mu1: 1.70   log-Likelihood: -169.40 \n",
      "21. p: 0.32, mu0: -0.29, mu1: 1.70   log-Likelihood: -169.40 \n",
      "22. p: 0.33, mu0: -0.29, mu1: 1.70   log-Likelihood: -169.40 \n",
      "23. p: 0.33, mu0: -0.29, mu1: 1.70   log-Likelihood: -169.40 \n",
      "24. p: 0.33, mu0: -0.29, mu1: 1.70   log-Likelihood: -169.40 \n",
      "25. p: 0.33, mu0: -0.29, mu1: 1.70   log-Likelihood: -169.40 \n",
      "26. p: 0.33, mu0: -0.29, mu1: 1.70   log-Likelihood: -169.40 \n",
      "27. p: 0.33, mu0: -0.29, mu1: 1.70   log-Likelihood: -169.40 \n",
      "28. p: 0.33, mu0: -0.29, mu1: 1.70   log-Likelihood: -169.40 \n",
      "29. p: 0.33, mu0: -0.29, mu1: 1.70   log-Likelihood: -169.40 \n",
      "30. p: 0.33, mu0: -0.29, mu1: 1.70   log-Likelihood: -169.40 \n"
     ]
    }
   ],
   "source": [
    "initial_theta = [0.1, -1, 1]\n",
    "\n",
    "num_of_iter = 30\n",
    "\n",
    "theta = initial_theta\n",
    "ll = log_likelihood(theta[0], theta[1], theta[2], data.Sample)\n",
    "print(\"Initial: {} log-Likelihood: {:.2f}\".format(theta, ll))\n",
    "\n",
    "for i in range(num_of_iter):\n",
    "    theta = update(theta[0], theta[1], theta[2], data.Sample)\n",
    "    ll = log_likelihood(theta[0], theta[1], theta[2], data.Sample)\n",
    "    print(\"{:2d}. p: {:.2f}, mu0: {:.2f}, mu1: {:.2f}   log-Likelihood: {:.2f} \".format(i+1,theta[0],theta[1],theta[2], ll))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that while the log likelihood is increasing on every step as it should. The parameter values are not getting close to the real one. In general, we cannot guarrantee that we will find the global maximum of the likelihood function, we can get stuck in a local maximum as well and therefore a lot depends on initialization. Also note that our estimator itself is random so we are also limited by the finite sample size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial: [0.3, 0, 2] log-Likelihood: -170.81\n",
      " 1. p: 0.28, mu0: 0.10, mu1: 2.03   log-Likelihood: -169.97 \n",
      " 2. p: 0.25, mu0: 0.14, mu1: 2.12   log-Likelihood: -169.39 \n",
      " 3. p: 0.23, mu0: 0.16, mu1: 2.22   log-Likelihood: -168.82 \n",
      " 4. p: 0.21, mu0: 0.19, mu1: 2.33   log-Likelihood: -168.27 \n",
      " 5. p: 0.19, mu0: 0.21, mu1: 2.45   log-Likelihood: -167.76 \n",
      " 6. p: 0.17, mu0: 0.23, mu1: 2.57   log-Likelihood: -167.30 \n",
      " 7. p: 0.16, mu0: 0.25, mu1: 2.69   log-Likelihood: -166.92 \n",
      " 8. p: 0.14, mu0: 0.27, mu1: 2.80   log-Likelihood: -166.63 \n",
      " 9. p: 0.13, mu0: 0.28, mu1: 2.89   log-Likelihood: -166.41 \n",
      "10. p: 0.13, mu0: 0.30, mu1: 2.98   log-Likelihood: -166.26 \n",
      "11. p: 0.12, mu0: 0.31, mu1: 3.05   log-Likelihood: -166.16 \n",
      "12. p: 0.11, mu0: 0.32, mu1: 3.11   log-Likelihood: -166.10 \n",
      "13. p: 0.11, mu0: 0.32, mu1: 3.15   log-Likelihood: -166.06 \n",
      "14. p: 0.11, mu0: 0.33, mu1: 3.18   log-Likelihood: -166.04 \n",
      "15. p: 0.10, mu0: 0.33, mu1: 3.21   log-Likelihood: -166.03 \n",
      "16. p: 0.10, mu0: 0.34, mu1: 3.23   log-Likelihood: -166.02 \n",
      "17. p: 0.10, mu0: 0.34, mu1: 3.24   log-Likelihood: -166.02 \n",
      "18. p: 0.10, mu0: 0.34, mu1: 3.25   log-Likelihood: -166.02 \n",
      "19. p: 0.10, mu0: 0.34, mu1: 3.26   log-Likelihood: -166.01 \n",
      "20. p: 0.10, mu0: 0.34, mu1: 3.27   log-Likelihood: -166.01 \n",
      "21. p: 0.10, mu0: 0.34, mu1: 3.27   log-Likelihood: -166.01 \n",
      "22. p: 0.10, mu0: 0.35, mu1: 3.28   log-Likelihood: -166.01 \n",
      "23. p: 0.10, mu0: 0.35, mu1: 3.28   log-Likelihood: -166.01 \n",
      "24. p: 0.10, mu0: 0.35, mu1: 3.28   log-Likelihood: -166.01 \n",
      "25. p: 0.10, mu0: 0.35, mu1: 3.28   log-Likelihood: -166.01 \n",
      "26. p: 0.10, mu0: 0.35, mu1: 3.28   log-Likelihood: -166.01 \n",
      "27. p: 0.10, mu0: 0.35, mu1: 3.28   log-Likelihood: -166.01 \n",
      "28. p: 0.10, mu0: 0.35, mu1: 3.28   log-Likelihood: -166.01 \n",
      "29. p: 0.10, mu0: 0.35, mu1: 3.28   log-Likelihood: -166.01 \n",
      "30. p: 0.10, mu0: 0.35, mu1: 3.28   log-Likelihood: -166.01 \n"
     ]
    }
   ],
   "source": [
    "initial_theta = [0.3, 0, 2]\n",
    "\n",
    "num_of_iter = 30\n",
    "\n",
    "theta = initial_theta\n",
    "ll = log_likelihood(theta[0], theta[1], theta[2], data.Sample)\n",
    "print(\"Initial: {} log-Likelihood: {:.2f}\".format(theta, ll))\n",
    "\n",
    "for i in range(num_of_iter):\n",
    "    theta = update(theta[0], theta[1], theta[2], data.Sample)\n",
    "    ll = log_likelihood(theta[0], theta[1], theta[2], data.Sample)\n",
    "    print(\"{:2d}. p: {:.2f}, mu0: {:.2f}, mu1: {:.2f}   log-Likelihood: {:.2f} \".format(i+1,theta[0],theta[1],theta[2], ll))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us try this on a much larger dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = 1000\n",
    "\n",
    "sample = []\n",
    "cl = []\n",
    "\n",
    "p = 0.3\n",
    "mu0 = 0\n",
    "mu1 = 2\n",
    "\n",
    "# There is probably a better way to do this...\n",
    "for i in range(n):\n",
    "    u = np.random.rand();\n",
    "    if (u <= 1-p):\n",
    "        sample.append(np.random.normal(mu0,1));\n",
    "        cl.append(0)\n",
    "    else:\n",
    "        sample.append(np.random.normal(mu1,1));\n",
    "        cl.append(1)\n",
    "        \n",
    "data = pd.DataFrame(list(zip(sample,cl)), columns = ['Sample', 'Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial: [0.3, 0, 2] log-Likelihood: -1667.82\n",
      " 1. p: 0.29, mu0: 0.05, mu1: 1.91   log-Likelihood: -1665.36 \n",
      " 2. p: 0.29, mu0: 0.06, mu1: 1.89   log-Likelihood: -1665.15 \n",
      " 3. p: 0.29, mu0: 0.06, mu1: 1.88   log-Likelihood: -1665.12 \n",
      " 4. p: 0.29, mu0: 0.07, mu1: 1.88   log-Likelihood: -1665.12 \n",
      " 5. p: 0.29, mu0: 0.07, mu1: 1.88   log-Likelihood: -1665.12 \n",
      " 6. p: 0.29, mu0: 0.07, mu1: 1.88   log-Likelihood: -1665.12 \n",
      " 7. p: 0.29, mu0: 0.07, mu1: 1.88   log-Likelihood: -1665.12 \n",
      " 8. p: 0.29, mu0: 0.07, mu1: 1.88   log-Likelihood: -1665.12 \n",
      " 9. p: 0.29, mu0: 0.07, mu1: 1.88   log-Likelihood: -1665.12 \n",
      "10. p: 0.29, mu0: 0.07, mu1: 1.88   log-Likelihood: -1665.12 \n",
      "11. p: 0.29, mu0: 0.07, mu1: 1.89   log-Likelihood: -1665.12 \n",
      "12. p: 0.29, mu0: 0.07, mu1: 1.89   log-Likelihood: -1665.12 \n",
      "13. p: 0.29, mu0: 0.07, mu1: 1.89   log-Likelihood: -1665.12 \n",
      "14. p: 0.29, mu0: 0.07, mu1: 1.89   log-Likelihood: -1665.12 \n",
      "15. p: 0.29, mu0: 0.07, mu1: 1.89   log-Likelihood: -1665.12 \n",
      "16. p: 0.29, mu0: 0.07, mu1: 1.89   log-Likelihood: -1665.12 \n",
      "17. p: 0.29, mu0: 0.07, mu1: 1.89   log-Likelihood: -1665.12 \n",
      "18. p: 0.29, mu0: 0.07, mu1: 1.89   log-Likelihood: -1665.12 \n",
      "19. p: 0.29, mu0: 0.07, mu1: 1.89   log-Likelihood: -1665.12 \n",
      "20. p: 0.29, mu0: 0.07, mu1: 1.89   log-Likelihood: -1665.12 \n",
      "21. p: 0.29, mu0: 0.07, mu1: 1.89   log-Likelihood: -1665.12 \n",
      "22. p: 0.29, mu0: 0.07, mu1: 1.89   log-Likelihood: -1665.12 \n",
      "23. p: 0.29, mu0: 0.07, mu1: 1.89   log-Likelihood: -1665.12 \n",
      "24. p: 0.29, mu0: 0.07, mu1: 1.89   log-Likelihood: -1665.12 \n",
      "25. p: 0.29, mu0: 0.07, mu1: 1.89   log-Likelihood: -1665.12 \n",
      "26. p: 0.29, mu0: 0.07, mu1: 1.89   log-Likelihood: -1665.12 \n",
      "27. p: 0.29, mu0: 0.07, mu1: 1.89   log-Likelihood: -1665.12 \n",
      "28. p: 0.29, mu0: 0.07, mu1: 1.89   log-Likelihood: -1665.12 \n",
      "29. p: 0.29, mu0: 0.07, mu1: 1.89   log-Likelihood: -1665.12 \n",
      "30. p: 0.29, mu0: 0.07, mu1: 1.89   log-Likelihood: -1665.12 \n"
     ]
    }
   ],
   "source": [
    "initial_theta = [0.3, 0, 2]\n",
    "\n",
    "num_of_iter = 30\n",
    "\n",
    "theta = initial_theta\n",
    "ll = log_likelihood(theta[0], theta[1], theta[2], data.Sample)\n",
    "print(\"Initial: {} log-Likelihood: {:.2f}\".format(theta, ll))\n",
    "\n",
    "for i in range(num_of_iter):\n",
    "    theta = update(theta[0], theta[1], theta[2], data.Sample)\n",
    "    ll = log_likelihood(theta[0], theta[1], theta[2], data.Sample)\n",
    "    print(\"{:2d}. p: {:.2f}, mu0: {:.2f}, mu1: {:.2f}   log-Likelihood: {:.2f} \".format(i+1,theta[0],theta[1],theta[2], ll))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better. Also note that the convergence is also much faster. Here is one more with the off initialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial: [0.1, -1, 1] log-Likelihood: -213.39\n",
      " 1. p: 0.32, mu0: -0.28, mu1: 1.72   log-Likelihood: -169.40 \n",
      " 2. p: 0.32, mu0: -0.28, mu1: 1.72   log-Likelihood: -169.40 \n",
      " 3. p: 0.32, mu0: -0.28, mu1: 1.71   log-Likelihood: -169.40 \n",
      " 4. p: 0.32, mu0: -0.28, mu1: 1.71   log-Likelihood: -169.40 \n",
      " 5. p: 0.32, mu0: -0.28, mu1: 1.71   log-Likelihood: -169.40 \n",
      " 6. p: 0.32, mu0: -0.29, mu1: 1.71   log-Likelihood: -169.40 \n",
      " 7. p: 0.32, mu0: -0.29, mu1: 1.71   log-Likelihood: -169.40 \n",
      " 8. p: 0.32, mu0: -0.29, mu1: 1.71   log-Likelihood: -169.40 \n",
      " 9. p: 0.32, mu0: -0.29, mu1: 1.71   log-Likelihood: -169.40 \n",
      "10. p: 0.32, mu0: -0.29, mu1: 1.71   log-Likelihood: -169.40 \n",
      "11. p: 0.32, mu0: -0.29, mu1: 1.71   log-Likelihood: -169.40 \n",
      "12. p: 0.32, mu0: -0.29, mu1: 1.71   log-Likelihood: -169.40 \n",
      "13. p: 0.32, mu0: -0.29, mu1: 1.71   log-Likelihood: -169.40 \n",
      "14. p: 0.32, mu0: -0.29, mu1: 1.71   log-Likelihood: -169.40 \n",
      "15. p: 0.32, mu0: -0.29, mu1: 1.70   log-Likelihood: -169.40 \n",
      "16. p: 0.32, mu0: -0.29, mu1: 1.70   log-Likelihood: -169.40 \n",
      "17. p: 0.32, mu0: -0.29, mu1: 1.70   log-Likelihood: -169.40 \n",
      "18. p: 0.32, mu0: -0.29, mu1: 1.70   log-Likelihood: -169.40 \n",
      "19. p: 0.32, mu0: -0.29, mu1: 1.70   log-Likelihood: -169.40 \n",
      "20. p: 0.32, mu0: -0.29, mu1: 1.70   log-Likelihood: -169.40 \n",
      "21. p: 0.32, mu0: -0.29, mu1: 1.70   log-Likelihood: -169.40 \n",
      "22. p: 0.33, mu0: -0.29, mu1: 1.70   log-Likelihood: -169.40 \n",
      "23. p: 0.33, mu0: -0.29, mu1: 1.70   log-Likelihood: -169.40 \n",
      "24. p: 0.33, mu0: -0.29, mu1: 1.70   log-Likelihood: -169.40 \n",
      "25. p: 0.33, mu0: -0.29, mu1: 1.70   log-Likelihood: -169.40 \n",
      "26. p: 0.33, mu0: -0.29, mu1: 1.70   log-Likelihood: -169.40 \n",
      "27. p: 0.33, mu0: -0.29, mu1: 1.70   log-Likelihood: -169.40 \n",
      "28. p: 0.33, mu0: -0.29, mu1: 1.70   log-Likelihood: -169.40 \n",
      "29. p: 0.33, mu0: -0.29, mu1: 1.70   log-Likelihood: -169.40 \n",
      "30. p: 0.33, mu0: -0.29, mu1: 1.70   log-Likelihood: -169.40 \n"
     ]
    }
   ],
   "source": [
    "initial_theta = [0.1, -1, 1]\n",
    "\n",
    "num_of_iter = 30\n",
    "\n",
    "theta = initial_theta\n",
    "ll = log_likelihood(theta[0], theta[1], theta[2], data.Sample)\n",
    "print(\"Initial: {} log-Likelihood: {:.2f}\".format(theta, ll))\n",
    "\n",
    "for i in range(num_of_iter):\n",
    "    theta = update(theta[0], theta[1], theta[2], data.Sample)\n",
    "    ll = log_likelihood(theta[0], theta[1], theta[2], data.Sample)\n",
    "    print(\"{:2d}. p: {:.2f}, mu0: {:.2f}, mu1: {:.2f}   log-Likelihood: {:.2f} \".format(i+1,theta[0],theta[1],theta[2], ll))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty good! On the other hand, look at what happens with really 'bad' initialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial: [0.1, 1, -1] log-Likelihood: -183.24\n",
      " 1. p: 0.18, mu0: 0.69, mu1: -1.13   log-Likelihood: -175.85 \n",
      " 2. p: 0.20, mu0: 0.71, mu1: -1.04   log-Likelihood: -175.48 \n",
      " 3. p: 0.22, mu0: 0.73, mu1: -0.98   log-Likelihood: -175.13 \n",
      " 4. p: 0.24, mu0: 0.76, mu1: -0.94   log-Likelihood: -174.78 \n",
      " 5. p: 0.26, mu0: 0.79, mu1: -0.89   log-Likelihood: -174.42 \n",
      " 6. p: 0.28, mu0: 0.83, mu1: -0.85   log-Likelihood: -174.07 \n",
      " 7. p: 0.30, mu0: 0.86, mu1: -0.81   log-Likelihood: -173.71 \n",
      " 8. p: 0.32, mu0: 0.90, mu1: -0.78   log-Likelihood: -173.36 \n",
      " 9. p: 0.34, mu0: 0.94, mu1: -0.74   log-Likelihood: -173.01 \n",
      "10. p: 0.37, mu0: 0.98, mu1: -0.71   log-Likelihood: -172.66 \n",
      "11. p: 0.39, mu0: 1.02, mu1: -0.68   log-Likelihood: -172.32 \n",
      "12. p: 0.41, mu0: 1.06, mu1: -0.65   log-Likelihood: -171.99 \n",
      "13. p: 0.43, mu0: 1.10, mu1: -0.62   log-Likelihood: -171.68 \n",
      "14. p: 0.45, mu0: 1.14, mu1: -0.59   log-Likelihood: -171.38 \n",
      "15. p: 0.47, mu0: 1.18, mu1: -0.57   log-Likelihood: -171.11 \n",
      "16. p: 0.49, mu0: 1.23, mu1: -0.54   log-Likelihood: -170.85 \n",
      "17. p: 0.51, mu0: 1.27, mu1: -0.52   log-Likelihood: -170.62 \n",
      "18. p: 0.53, mu0: 1.31, mu1: -0.50   log-Likelihood: -170.42 \n",
      "19. p: 0.54, mu0: 1.34, mu1: -0.48   log-Likelihood: -170.24 \n",
      "20. p: 0.56, mu0: 1.38, mu1: -0.46   log-Likelihood: -170.08 \n",
      "21. p: 0.57, mu0: 1.41, mu1: -0.44   log-Likelihood: -169.95 \n",
      "22. p: 0.58, mu0: 1.44, mu1: -0.42   log-Likelihood: -169.84 \n",
      "23. p: 0.59, mu0: 1.47, mu1: -0.41   log-Likelihood: -169.75 \n",
      "24. p: 0.60, mu0: 1.50, mu1: -0.40   log-Likelihood: -169.68 \n",
      "25. p: 0.61, mu0: 1.52, mu1: -0.38   log-Likelihood: -169.62 \n",
      "26. p: 0.62, mu0: 1.54, mu1: -0.37   log-Likelihood: -169.57 \n",
      "27. p: 0.63, mu0: 1.56, mu1: -0.36   log-Likelihood: -169.53 \n",
      "28. p: 0.63, mu0: 1.58, mu1: -0.35   log-Likelihood: -169.50 \n",
      "29. p: 0.64, mu0: 1.60, mu1: -0.35   log-Likelihood: -169.48 \n",
      "30. p: 0.64, mu0: 1.61, mu1: -0.34   log-Likelihood: -169.46 \n",
      "31. p: 0.65, mu0: 1.62, mu1: -0.33   log-Likelihood: -169.44 \n",
      "32. p: 0.65, mu0: 1.63, mu1: -0.33   log-Likelihood: -169.43 \n",
      "33. p: 0.65, mu0: 1.64, mu1: -0.32   log-Likelihood: -169.42 \n",
      "34. p: 0.66, mu0: 1.65, mu1: -0.32   log-Likelihood: -169.42 \n",
      "35. p: 0.66, mu0: 1.65, mu1: -0.31   log-Likelihood: -169.41 \n",
      "36. p: 0.66, mu0: 1.66, mu1: -0.31   log-Likelihood: -169.41 \n",
      "37. p: 0.66, mu0: 1.67, mu1: -0.31   log-Likelihood: -169.41 \n",
      "38. p: 0.66, mu0: 1.67, mu1: -0.31   log-Likelihood: -169.40 \n",
      "39. p: 0.67, mu0: 1.68, mu1: -0.30   log-Likelihood: -169.40 \n",
      "40. p: 0.67, mu0: 1.68, mu1: -0.30   log-Likelihood: -169.40 \n",
      "41. p: 0.67, mu0: 1.68, mu1: -0.30   log-Likelihood: -169.40 \n",
      "42. p: 0.67, mu0: 1.68, mu1: -0.30   log-Likelihood: -169.40 \n",
      "43. p: 0.67, mu0: 1.69, mu1: -0.30   log-Likelihood: -169.40 \n",
      "44. p: 0.67, mu0: 1.69, mu1: -0.30   log-Likelihood: -169.40 \n",
      "45. p: 0.67, mu0: 1.69, mu1: -0.30   log-Likelihood: -169.40 \n",
      "46. p: 0.67, mu0: 1.69, mu1: -0.29   log-Likelihood: -169.40 \n",
      "47. p: 0.67, mu0: 1.69, mu1: -0.29   log-Likelihood: -169.40 \n",
      "48. p: 0.67, mu0: 1.69, mu1: -0.29   log-Likelihood: -169.40 \n",
      "49. p: 0.67, mu0: 1.70, mu1: -0.29   log-Likelihood: -169.40 \n",
      "50. p: 0.67, mu0: 1.70, mu1: -0.29   log-Likelihood: -169.40 \n"
     ]
    }
   ],
   "source": [
    "initial_theta = [0.1, 1, -1]\n",
    "\n",
    "num_of_iter = 50\n",
    "\n",
    "theta = initial_theta\n",
    "ll = log_likelihood(theta[0], theta[1], theta[2], data.Sample)\n",
    "print(\"Initial: {} log-Likelihood: {:.2f}\".format(theta, ll))\n",
    "\n",
    "for i in range(num_of_iter):\n",
    "    theta = update(theta[0], theta[1], theta[2], data.Sample)\n",
    "    ll = log_likelihood(theta[0], theta[1], theta[2], data.Sample)\n",
    "    print(\"{:2d}. p: {:.2f}, mu0: {:.2f}, mu1: {:.2f}   log-Likelihood: {:.2f} \".format(i+1,theta[0],theta[1],theta[2], ll))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, this is going for the 'other' maximum of the likelihood function where the roles of the two population are switched. Note that the model density is unchanged under this symmetry: $(0.3, 0, 2)$ gives the exact same dsns\n",
    "\n",
    "Feel free to play around and get a feel for how the algorithm behaves with different initialization, sample sizes, and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
